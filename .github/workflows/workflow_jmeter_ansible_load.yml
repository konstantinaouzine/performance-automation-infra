name: Load Test - JMeter Ansible
run-name: "JMeter: ${{ github.event.inputs.test_file || 'test_app_load_only.jmx' }}"

on:
  workflow_dispatch:
    inputs:
      test_file:
        description: 'JMX file name (with no path, can be without extension)'
        required: true
        default: 'test_app_load_only.jmx'
      email_to:
        description: 'Emails for report (use comma)'
        required: false
        default: ''

jobs:
  run-ansible-jmeter:
    runs-on: [self-hosted, ansible, jmeter]
    timeout-minutes: 40
    env:
      # Use GitHub expression so путь разворачивается корректно, не буквальная строка
      LOGS_DIR: ${{ github.workspace }}/jmeter_logs
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Load secret environment
        run: |
          # Export secrets (may be empty) into environment for later steps
          echo "GRAFANA_URL=${{ secrets.GRAFANA_URL }}" >> $GITHUB_ENV
          echo "GRAFANA_API_TOKEN=${{ secrets.GRAFANA_API_TOKEN }}" >> $GITHUB_ENV
          echo "GRAFANA_SPECS=${{ secrets.GRAFANA_SPECS }}" >> $GITHUB_ENV
          echo "SENDGRID_API_KEY=${{ secrets.SENDGRID_API_KEY }}" >> $GITHUB_ENV
          echo "SENDGRID_FROM=${{ secrets.SENDGRID_FROM }}" >> $GITHUB_ENV

      - name: Capture test start time
        id: mark_start
        run: |
          echo "start_ms=$(date +%s%3N)" >> $GITHUB_OUTPUT
          echo "Captured test start time: $(grep start_ms $GITHUB_OUTPUT | cut -d= -f2)"

      - name: Generate deterministic run id
        id: run_id
        run: |
          set -euo pipefail
          # Pattern: YYYYMMDDThhmmss_RUNNUMBER_SHAShort
          GEN_ID="$(date -u +%Y%m%dT%H%M%S)_${GITHUB_RUN_NUMBER}_${GITHUB_SHA::7}"
          echo "Generated RUN_ID=$GEN_ID"
          echo "RUN_ID=$GEN_ID" >> $GITHUB_ENV
          echo "run_id=$GEN_ID" >> $GITHUB_OUTPUT

      - name: Debug workspace & list tests
        run: |
          echo "GITHUB_WORKSPACE=$GITHUB_WORKSPACE"
          ls -1 $GITHUB_WORKSPACE/ansible_jmeter/tests || { echo 'tests dir missing'; exit 1; }

      - name: Ensure Ansible
        run: |
          if ! command -v ansible-playbook >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y ansible python3-jmespath
          fi
          ansible --version

      - name: Ensure kubectl
        run: |
          if ! command -v kubectl >/dev/null 2>&1; then
            curl -sLO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
            chmod +x kubectl
            sudo mv kubectl /usr/local/bin/
          fi
          kubectl version --client --output=yaml || true

      - name: Activate minikube docker-env (if available)
        run: |
          if command -v minikube >/dev/null 2>&1; then
            echo "Activating minikube docker-env";
            eval $(minikube docker-env)
            docker info 2>/dev/null | grep -i 'server version' || true
          else
            echo "minikube not found - using default docker context"
          fi

      - name: Run Ansible JMeter playbook
        id: ansible_run
        run: |
          set -e
          TEST_FILE="${{ github.event.inputs.test_file }}"
          if [[ "$TEST_FILE" != *.jmx ]]; then TEST_FILE="${TEST_FILE}.jmx"; fi
          mkdir -p "$LOGS_DIR"
          echo "Using LOGS_DIR=$LOGS_DIR"
          kubectl delete job -n load -l app=jmeter-loadgen --ignore-not-found || true
          ansible-playbook ansible_jmeter/jmeter_play.yml \
            -e jmeter_test_file="$TEST_FILE" \
            -e jmeter_local_logs_dir="$LOGS_DIR" || { echo 'Playbook failed'; exit 1; }
          echo 'After playbook directory tree:'
          find "$LOGS_DIR" -maxdepth 2 -type f -name run.meta -printf 'META %p\n' || true

      - name: Discover run dir from metadata (fallback)
        id: discover
        run: |
          set -e
          if [ ! -d "$LOGS_DIR" ]; then echo 'No logs dir (discover)'; exit 0; fi
          META=$(find "$LOGS_DIR" -maxdepth 2 -type f -name run.meta | head -1 || true)
          if [ -n "$META" ]; then
            RUN_DIR=$(dirname "$META")
            echo "Found run dir via run.meta: $RUN_DIR"
            echo "latest_dir=$RUN_DIR" >> $GITHUB_OUTPUT
          else
            echo 'No run.meta found yet'
          fi

      - name: Locate latest run directory
        id: locate
        run: |
          set -e
          if [ ! -d "$LOGS_DIR" ]; then echo "No logs dir" >&2; exit 1; fi
          echo 'Existing entries:'
          ls -l "$LOGS_DIR" || true
          # Prefer metadata-derived value first
          if [ -n "${{ steps.discover.outputs.latest_dir }}" ]; then
            LATEST='${{ steps.discover.outputs.latest_dir }}'
          else
            LATEST=$(ls -1dt "$LOGS_DIR"/*/ 2>/dev/null | head -1 | sed 's:/*$::')
          fi
          # Fallback to symlink 'latest'
          if [ -z "$LATEST" ] && [ -L "$LOGS_DIR/latest" ]; then
            LATEST=$(readlink -f "$LOGS_DIR/latest")
          fi
          # Fallback to any directory if glob with slash failed
          if [ -z "$LATEST" ]; then
            LATEST=$(find "$LOGS_DIR" -maxdepth 1 -mindepth 1 -type d | sort -r | head -1)
          fi
          echo "Latest run dir: $LATEST"
          echo "latest_dir=$LATEST" >> $GITHUB_OUTPUT
          ls -1 "$LATEST" || true
          # Drop a marker file with deterministic RUN_ID if available
          if [ -n "${{ steps.run_id.outputs.run_id }}" ] && [ -d "$LATEST" ]; then
            echo "${{ steps.run_id.outputs.run_id }}" > "$LATEST/run.id" || true
          fi

      - name: SLA & summary
        id: sla
        run: |
          set -euo pipefail
          LATEST='${{ steps.locate.outputs.latest_dir }}'
          if [ -z "$LATEST" ] || [ ! -d "$LATEST" ]; then echo 'No run dir'; exit 1; fi
          JTL="$LATEST/results.jtl"
          if [ ! -f "$JTL" ]; then echo 'No results.jtl found'; exit 1; fi
          echo "Using LATEST=$LATEST"
          echo '--- LAST 10 LINES ---'
          tail -n 10 "$JTL" || true

          echo '--- JTL HEADER ---'
          head -1 "$JTL" || true

          # Detect XML vs CSV
          if grep -q '<sample ' "$JTL"; then
            FORMAT=xml
            TOTAL=$(grep -c '<sample ' "$JTL" || echo 0)
            ERR=$(grep -c '<sample [^>]*s="false"' "$JTL" || echo 0)
          else
            FORMAT=csv
            # Strip CR, ignore blank lines. Assume first line header if contains 'success'
            HEADER=$(head -1 "$JTL" | tr -d '\r') || HEADER=""
            if echo "$HEADER" | grep -qi 'success'; then
              # Count data lines (exclude header & blanks)
              TOTAL=$(tail -n +2 "$JTL" | tr -d '\r' | grep -cv '^$' || echo 0)
              ERR=$(tail -n +2 "$JTL" | tr -d '\r' | grep -c ',false,' || echo 0)
            else
              # Fallback: count all non-empty lines
              TOTAL=$(grep -cv '^$' "$JTL" || echo 0)
              ERR=0
            fi
          fi

          # Sanitize numeric (remove anything after first non-digit)
          TOTAL=$(echo "$TOTAL" | sed 's/[^0-9].*$//')
          ERR=$(echo "$ERR" | sed 's/[^0-9].*$//')

          if [ -z "$TOTAL" ] || ! echo "$TOTAL" | grep -Eq '^[0-9]+$'; then TOTAL=0; fi
          if [ -z "$ERR" ]   || ! echo "$ERR"   | grep -Eq '^[0-9]+$'; then ERR=0; fi

          if [ "$TOTAL" -eq 0 ]; then
            PCT="0.00"
          else
            # Use awk for percentage
            PCT=$(awk -v e="$ERR" -v t="$TOTAL" 'BEGIN{ if(t==0){print "0.00"} else {printf "%.2f", (e/t*100)} }') || PCT="0.00"
          fi

          echo "Format: $FORMAT Total=$TOTAL Errors=$ERR Error%=$PCT"
          echo "Samples: $TOTAL Errors: $ERR ($PCT%)"
          # Write step outputs (overwrite to avoid stray lines)
          {
            echo "total=$TOTAL"
            echo "errors=$ERR"
            echo "error_pct=$PCT"
            echo "format=$FORMAT"
          } > "$GITHUB_OUTPUT"

          # Sanitize: keep only valid key=value lines (defensive against stray output)
          TMP_SAN=$(mktemp)
          grep -E '^[A-Za-z0-9_]+=' "$GITHUB_OUTPUT" > "$TMP_SAN" || true
          mv "$TMP_SAN" "$GITHUB_OUTPUT"
          echo '--- GITHUB_OUTPUT CONTENT ---'
          cat "$GITHUB_OUTPUT"

          # SLA: fail if >5% (same reversed awk exit logic)
          if awk -v p="$PCT" 'BEGIN{exit (p>5)?0:1}'; then
            echo 'SLA FAIL >5% errors' >&2
            exit 1
          else
            echo 'SLA OK'
          fi

      - name: Capture test end time
        id: mark_end
        if: ${{ always() }}
        run: |
          echo "end_ms=$(date +%s%3N)" >> $GITHUB_OUTPUT
          echo "Captured test end time: $(grep end_ms $GITHUB_OUTPUT | cut -d= -f2)"

      - name: Aggregate & persist run metrics (Prometheus + JMeter -> Postgres)
        id: aggregate
        if: ${{ always() }}
        env:
          GIT_COMMIT: ${{ github.sha }}
          GIT_BRANCH: ${{ github.ref_name }}
        run: |
          set -euo pipefail
          RUN_DIR='${{ steps.locate.outputs.latest_dir }}'
          if [ -z "$RUN_DIR" ] || [ ! -d "$RUN_DIR" ]; then
            echo "No run directory discovered -> skip aggregation" >&2
            exit 0
          fi
          START_MS='${{ steps.mark_start.outputs.start_ms }}'
            END_MS='${{ steps.mark_end.outputs.end_ms }}'
          if [ -z "$START_MS" ]; then echo "Missing START_MS -> skip"; exit 0; fi
          if [ -z "$END_MS" ]; then END_MS=$(date +%s%3N); fi
          START_TS=$(( START_MS / 1000 ))
          END_TS=$(( END_MS / 1000 ))
          if [ "$END_TS" -le "$START_TS" ]; then
            echo "END_TS <= START_TS (adjusting END_TS = START_TS+1)" >&2
            END_TS=$(( START_TS + 1 ))
          fi
          # Prefer deterministic RUN_ID from earlier step / marker file
          if [ -n "${RUN_ID:-}" ]; then
            : # already exported via GITHUB_ENV
          elif [ -f "$RUN_DIR/run.id" ]; then
            RUN_ID=$(cat "$RUN_DIR/run.id" | tr -d '\n' || true)
          else
            RUN_ID=$(basename "$RUN_DIR" || true)
          fi
          if [ -z "$RUN_ID" ]; then RUN_ID="run_${{ github.run_id }}"; fi
          echo "Using RUN_ID=$RUN_ID window=$START_TS..$END_TS";

          # --- Port-forward Prometheus (if PROM_URL not provided) ---
          if [ -z "${PROM_URL:-}" ]; then
            PROM_NS=${PROM_NAMESPACE:-monitoring}
            PROM_SVC=${PROM_SERVICE:-prometheus-server}
            echo "Port-forwarding Prometheus svc/${PROM_SVC} in ${PROM_NS} to 9090";
            set +e
            kubectl port-forward -n "$PROM_NS" svc/"$PROM_SVC" 9090:9090 >/tmp/prom_pf.log 2>&1 &
            PF_PROM=$!
            # Wait up to ~15s
            for i in $(seq 1 30); do
              if curl -sf http://127.0.0.1:9090/-/ready >/dev/null 2>&1; then break; fi
              sleep 0.5
            done
            if ! curl -sf http://127.0.0.1:9090/-/ready >/dev/null 2>&1; then
              echo "WARN: Prometheus not reachable on 9090 (continuing, aggregation may fail)" >&2
            fi
            set -e
            export PROM_URL="http://127.0.0.1:9090"
          else
            echo "PROM_URL preset: $PROM_URL"
          fi

          # --- Port-forward Metrics Postgres (if PG_HOST not reachable) ---
          PG_LOCAL_PORT=${PG_PORT:-15432}
          : "${PG_HOST:=127.0.0.1}"
          if ! (echo > /dev/tcp/$PG_HOST/$PG_LOCAL_PORT) >/dev/null 2>&1; then
            MP_NS=${METRICS_PG_NAMESPACE:-perf-agg}
            MP_SVC=${METRICS_PG_SERVICE:-metrics-postgres-postgresql}
            echo "Port-forwarding metrics Postgres svc/${MP_SVC} in ${MP_NS} to ${PG_LOCAL_PORT}";
            kubectl port-forward -n "$MP_NS" svc/"$MP_SVC" ${PG_LOCAL_PORT}:5432 >/tmp/pg_pf.log 2>&1 &
            PF_PG=$!
            for i in $(seq 1 30); do
              if (echo > /dev/tcp/127.0.0.1/${PG_LOCAL_PORT}) >/dev/null 2>&1; then break; fi
              sleep 0.5
            done
            if ! (echo > /dev/tcp/127.0.0.1/${PG_LOCAL_PORT}) >/dev/null 2>&1; then
              echo "WARN: metrics Postgres not reachable on ${PG_LOCAL_PORT}" >&2
            fi
            export PG_HOST=127.0.0.1
            export PG_PORT=${PG_LOCAL_PORT}
          fi

          # --- Prepare Python env ---
          if ! command -v python3 >/dev/null 2>&1; then
            echo "Python3 not installed on runner" >&2; exit 0;
          fi
          python3 -m venv agg-env || true
          . agg-env/bin/activate || true
          python -m pip install --upgrade pip >/dev/null 2>&1 || true
          pip install --no-cache-dir requests psycopg2-binary >/dev/null

          # JMeter JTL path
          JTL="$RUN_DIR/results.jtl"
          if [ ! -f "$JTL" ]; then
            echo "WARN: JTL file missing ($JTL) – continuing without JMeter metrics" >&2
            unset JTL
          fi

          # Validate JTL freshness if present
          if [ -n "${JTL:-}" ]; then
            if command -v stat >/dev/null 2>&1; then
              JTL_MTIME=$(stat -c %Y "$JTL" 2>/dev/null || echo 0)
              # Accept mtime inside window +/- 10 minutes tolerance
              LOW_BOUND=$(( START_TS - 600 ))
              HIGH_BOUND=$(( END_TS + 600 ))
              if [ "$JTL_MTIME" -lt "$LOW_BOUND" ] || [ "$JTL_MTIME" -gt "$HIGH_BOUND" ]; then
                echo "WARN: JTL mtime ($JTL_MTIME) outside expected window ${LOW_BOUND}..${HIGH_BOUND}" >&2
              fi
            fi
            # Attempt to parse last timestamp from CSV (millis in first column) if header present
            LAST_TS_MS=$(tail -n 200 "$JTL" | grep -v '^$' | awk -F',' 'NR==1{h=$0} NR>1 && $1 ~ /^[0-9]+$/ {ts=$1} END{ if(ts!="") print ts }')
            if [ -n "$LAST_TS_MS" ]; then
              LAST_TS=$(( LAST_TS_MS / 1000 ))
              if [ $LAST_TS -lt $(( START_TS - 600 )) ] || [ $LAST_TS -gt $(( END_TS + 600 )) ]; then
                echo "WARN: Last sample timestamp ($LAST_TS) outside run window ($START_TS..$END_TS)" >&2
              fi
            fi
          fi

          export RUN_ID START_TS END_TS
          export PG_DB=${PG_DB:-perf_agg}
          export PG_USER=${PG_USER:-agg_user}
          export PG_PASSWORD=${PG_PASSWORD:-agg_pass}
          export JMETER_JTL_PATH="${JTL:-}"
          export APP_REQUIRE_APPLICATION=${APP_REQUIRE_APPLICATION:-true}
          # Allow selectors override via secrets/vars if desired
          : "${APP_JOB_REGEX:='(app/springboot-app|springboot-app)'}"

          echo "Running aggregation script..."
          set +e
          python scripts/aggregate_metrics.py | tee agg_output.log
          AGG_RC=${PIPESTATUS[0]}
          set -e
          if [ $AGG_RC -ne 0 ]; then
            echo "Aggregation script exited with code $AGG_RC" >&2
          fi

          # Extract JSON summary
          awk '/JSON_SUMMARY_START/{f=1;next}/JSON_SUMMARY_END/{f=0}f' agg_output.log > aggregated_metrics.json || true
          if [ -s aggregated_metrics.json ]; then
            echo "Aggregated metrics JSON:"; cat aggregated_metrics.json
          else
            echo "No aggregated_metrics.json produced" >&2
          fi

          # Cleanup port-forwards
          if [ -n "${PF_PROM:-}" ] && ps -p ${PF_PROM} >/dev/null 2>&1; then kill ${PF_PROM} || true; fi
          if [ -n "${PF_PG:-}" ] && ps -p ${PF_PG}  >/dev/null 2>&1; then kill ${PF_PG}  || true; fi

          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT || true

      - name: (Optional) Grafana port-forward
        id: grafana_pf
        if: ${{ always() }}
        run: |
          set -euo pipefail
          GRAFANA_URL_PRESET='${GRAFANA_URL:-}'
          if [ -n "${GRAFANA_URL_PRESET}" ]; then
            echo "Preset GRAFANA_URL provided -> skip port-forward";
            echo "grafana_url=${GRAFANA_URL_PRESET}" >> $GITHUB_OUTPUT
            exit 0;
          fi
          # Determine namespace & service (defaults)
          NS=${GRAFANA_NAMESPACE:-monitoring}
          SVC=${GRAFANA_SERVICE:-grafana}
          PORT=${GRAFANA_SERVICE_PORT:-3000}
          echo "Attempting kubectl port-forward svc/${SVC} ${PORT} -> 3000 in namespace ${NS}";
          # Start background port-forward
          kubectl port-forward -n "$NS" svc/"$SVC" 3000:"$PORT" >/tmp/grafana_pf.log 2>&1 &
          PF_PID=$!
          echo "Started port-forward PID=$PF_PID"
          echo "pf_pid=$PF_PID" >> $GITHUB_OUTPUT
          # Wait for readiness (max ~15s)
          for i in $(seq 1 30); do
            if curl -sf http://127.0.0.1:3000/api/health >/dev/null 2>&1; then
              echo "Grafana reachable via localhost:3000"; break; fi
            sleep 0.5
          done
          if ! curl -sf http://127.0.0.1:3000/api/health >/dev/null 2>&1; then
            echo " WARNING: Grafana port-forward not reachable (continuing, snapshots may fail)" >&2
          fi
          echo "grafana_url=http://127.0.0.1:3000" >> $GITHUB_OUTPUT
          # Export to subsequent steps via GITHUB_ENV so snapshot step can reference if secret absent
          echo "GRAFANA_URL=http://127.0.0.1:3000" >> $GITHUB_ENV

      - name: Grafana snapshots
        id: grafana_snapshots
        if: ${{ always() }}
        run: |
          set -euo pipefail
          GRAFANA_URL_PRESET="${GRAFANA_URL:-}"
          GRAFANA_API_TOKEN="${GRAFANA_API_TOKEN:-}"
          if [ -z "$GRAFANA_URL_PRESET" ]; then
            GRAFANA_URL='${{ steps.grafana_pf.outputs.grafana_url }}'
          else
            GRAFANA_URL="$GRAFANA_URL_PRESET"
          fi
          GRAFANA_SPECS="${GRAFANA_SPECS:-}"
          echo "(debug) Raw GRAFANA_SPECS=$GRAFANA_SPECS"
          : "${GRAFANA_RENDER_WIDTH:=1500}"; : "${GRAFANA_RENDER_HEIGHT:=850}"; : "${GRAFANA_RENDER_DELAY_SECS:=3}"; : "${GRAFANA_COMBINE_PANELS:=true}"; : "${GRAFANA_COMBINE_GRID:=2x2}";
          if [ -z "${GRAFANA_URL:-}" ] || [ -z "${GRAFANA_API_TOKEN:-}" ]; then
            echo "Grafana creds not set -> skip"; exit 0; fi
          START='${{ steps.mark_start.outputs.start_ms }}'
          END='${{ steps.mark_end.outputs.end_ms }}'
          if [ -z "$START" ]; then echo "Missing start time"; exit 0; fi
          if [ -z "$END" ]; then echo "Missing end time (maybe earlier failure) -> use now"; END=$(date +%s%3N); fi
          echo "GRAFANA_SPECS length=${#GRAFANA_SPECS}"
          if [ -z "${GRAFANA_SPECS}" ]; then echo "No GRAFANA_SPECS provided -> skip"; exit 0; fi
          OUT_DIR="grafana_snapshots"
          mkdir -p "$OUT_DIR"
          authHeader="Authorization: Bearer ${GRAFANA_API_TOKEN}"
          IFS=';' read -ra DASH <<< "$GRAFANA_SPECS"
          for spec in "${DASH[@]}"; do
            # Throttle between dashboards to reduce renderer load
            sleep "$GRAFANA_RENDER_DELAY_SECS"
            spec=$(echo "$spec" | xargs)
            [ -z "$spec" ] && continue
            uid=${spec%%:*}
            panels_part=""
            if [[ "$spec" == *:* ]]; then
              panels_part=${spec#*:}
            fi
            echo "Processing dashboard $uid"
            # Get slug
            DASH_JSON=$(curl -sf -H "$authHeader" "$GRAFANA_URL/api/dashboards/uid/$uid" || true)
            if [ -z "$DASH_JSON" ]; then echo "Failed to fetch dashboard $uid"; continue; fi
            slug=$(echo "$DASH_JSON" | python3 -c 'import sys,json; d=json.load(sys.stdin);print(d.get("meta",{}).get("slug","dashboard"))')
            if [ -z "$panels_part" ]; then
              # Full dashboard render with simple retry (up to 3 attempts)
              file="$OUT_DIR/${uid}_full.png"
              url="$GRAFANA_URL/render/d/$uid/$slug?from=$START&to=$END&width=${GRAFANA_RENDER_WIDTH}&height=${GRAFANA_RENDER_HEIGHT}&tz=UTC"
              echo "Downloading full dashboard -> $file"
              success=0
              for attempt in 1 2 3; do
                if curl -sf -H "$authHeader" -o "$file" "$url"; then
                  success=1; break
                else
                  echo "Attempt $attempt failed for $uid; retrying...";
                  sleep 2
                fi
              done
              if [ $success -ne 1 ]; then
                echo "Failed dashboard $uid after retries"
                rm -f "$file"
                # If the dashboard is heavy (e.g., app-perf) try fallback: render subset of key panels
                if [ "$uid" = "app-perf" ]; then
                  echo "Fallback: rendering key panels for $uid"
                  KEY_PANELS="101,104,111,141,151"
                  IFS=',' read -ra KP <<< "$KEY_PANELS"
                  for kp in "${KP[@]}"; do
                    kp=$(echo "$kp"|xargs)
                    [ -z "$kp" ] && continue
                    f2="$OUT_DIR/${uid}_panel_${kp}_fallback.png"
                    u2="$GRAFANA_URL/render/d-solo/$uid/$slug?panelId=$kp&from=$START&to=$END&width=${GRAFANA_RENDER_WIDTH}&height=500&tz=UTC"
                    echo "Downloading fallback panel $kp -> $f2"
                    curl -sf -H "$authHeader" -o "$f2" "$u2" || { echo "Failed fallback panel $kp"; rm -f "$f2"; }
                  done
                fi
              fi
            else
              IFS=',' read -ra PANELS <<< "$panels_part"
              PANEL_FILES=()
              for pid in "${PANELS[@]}"; do
                pid=$(echo "$pid" | xargs)
                [ -z "$pid" ] && continue
                file="$OUT_DIR/${uid}_panel_${pid}.png"
                url="$GRAFANA_URL/render/d-solo/$uid/$slug?panelId=$pid&from=$START&to=$END&width=1600&height=500&tz=UTC"
                echo "Downloading panel $pid -> $file"
                curl -sf -H "$authHeader" -o "$file" "$url" || { echo "Failed panel $pid of $uid"; rm -f "$file"; }
                [ -f "$file" ] && PANEL_FILES+=("$file")
              done
              # Combine panels if requested and we have at least 2 files
              if [ "${GRAFANA_COMBINE_PANELS,,}" = "true" ] && [ ${#PANEL_FILES[@]} -ge 1 ]; then
                if ! command -v montage >/dev/null 2>&1; then
                  echo "Installing ImageMagick (montage) for panel combination";
                  sudo apt-get update -y && sudo apt-get install -y imagemagick || echo "ImageMagick install failed (continuing without combine)";
                fi
                if command -v montage >/dev/null 2>&1; then
                  GRID=${GRAFANA_COMBINE_GRID:-}
                  COLS=""; ROWS=""
                  if echo "$GRID" | grep -Eq '^[0-9]+x[0-9]+$'; then
                    COLS=${GRID%x*}; ROWS=${GRID#*x}
                  elif echo "$GRID" | grep -Eq '^[0-9]+x$'; then
                    COLS=${GRID%x}; ROWS=0
                  fi
                  # If rows not specified compute rows = ceil(n/COLS)
                  if [ -n "$COLS" ] && [ -z "$ROWS" -o "$ROWS" = "0" ]; then
                    N=${#PANEL_FILES[@]}
                    ROWS=$(( (N + COLS - 1) / COLS ))
                  fi
                  COMBINED="$OUT_DIR/${uid}_combined.png"
                  echo "Combining ${#PANEL_FILES[@]} panels into $COMBINED (grid ${COLS}x${ROWS})"
                  # Use +geometry to avoid gaps; background white for stats
                  montage "${PANEL_FILES[@]}" -tile ${COLS}x${ROWS} -geometry +0+0 -background '#ffffff' "$COMBINED" || echo "Montage combine failed"
                else
                  echo "montage not available; skipping combine"
                fi
              fi
            fi
          done
          echo "Snapshot files:"; ls -1 "$OUT_DIR" || echo "(none)"
          # Detect placeholder images (renderer missing) by checking small PNGs containing the error text
          for f in "$OUT_DIR"/*.png; do
            [ -e "$f" ] || continue
            SIZE=$(stat -c%s "$f" || echo 0)
            if [ "$SIZE" -lt 15000 ]; then
              if strings "$f" 2>/dev/null | grep -qi 'No image renderer'; then
                echo "WARNING: Placeholder image (renderer not active yet): $f" >&2
              fi
            fi
          done

      - name: Cleanup port-forward
        if: ${{ always() && steps.grafana_pf.outputs.pf_pid != '' }}
        run: |
          set -euo pipefail
          PID='${{ steps.grafana_pf.outputs.pf_pid }}'
          if [ -n "$PID" ] && ps -p "$PID" >/dev/null 2>&1; then
            echo "Killing port-forward PID $PID";
            kill "$PID" || true
          else
            echo "No active port-forward PID to kill"
          fi

      - name: Email report (SendGrid API)
        if: ${{ always() && github.event.inputs.email_to != '' }}
        run: |
          set -euo pipefail
          SENDGRID_API_KEY='${SENDGRID_API_KEY:-}'
          SENDGRID_FROM_SECRET='${SENDGRID_FROM:-}'
          SMTP_FROM_SECRET='' # not provided via inputs now
          if [ -n "$SENDGRID_FROM_SECRET" ]; then SENDGRID_FROM="$SENDGRID_FROM_SECRET"; elif [ -n "$SMTP_FROM_SECRET" ]; then SENDGRID_FROM="$SMTP_FROM_SECRET"; else SENDGRID_FROM='loadtest-bot@example.com'; fi
          export SENDGRID_API_KEY SENDGRID_FROM
          # 1. Guard API key
          if [ -z "${SENDGRID_API_KEY:-}" ]; then
            echo 'SENDGRID_API_KEY empty -> skip email'
            exit 0
          fi

          # 2. Locate run dir
          LATEST='${{ steps.locate.outputs.latest_dir }}'
          if [ -z "$LATEST" ] || [ ! -d "$LATEST" ]; then
            echo "No latest run dir -> skip"
            exit 0
          fi

          # 3. Normalize recipients
          RAW='${{ github.event.inputs.email_to }}'
          NORMALIZED=$(echo "$RAW" | tr '\n' ',' | tr ' ' ',' | sed 's/,,*/,/g; s/^,//; s/,$//')
          if [ -z "$NORMALIZED" ]; then
            echo "No recipients after normalization"
            exit 0
          fi

          export NORMALIZED LATEST
          export JOB_STATUS='${{ job.status }}'
          export SAMPLES='${{ steps.sla.outputs.total }}'
          export ERRORS='${{ steps.sla.outputs.errors }}'
          export ERROR_PCT='${{ steps.sla.outputs.error_pct }}'
          export FORMAT='${{ steps.sla.outputs.format }}'

          echo "Building payload.json (no attachments)"

          python3 - <<'PY'
          import os, json, sys

          recips = [r.strip() for r in os.environ['NORMALIZED'].split(',') if r.strip()]
          if not recips:
              print("No valid recipients")
              sys.exit(0)

          latest = os.environ['LATEST']
          repo   = os.environ.get('GITHUB_REPOSITORY','')
          workflow = os.environ.get('GITHUB_WORKFLOW','')
          run_number = os.environ.get('GITHUB_RUN_NUMBER','')
          run_id  = os.environ.get('GITHUB_RUN_ID','')
          server  = os.environ.get('GITHUB_SERVER_URL','https://github.com')
          status  = os.environ.get('JOB_STATUS','unknown')
          samples = os.environ.get('SAMPLES','0')
          errors  = os.environ.get('ERRORS','0')
          error_pct = os.environ.get('ERROR_PCT','0')
          fmt = os.environ.get('FORMAT','csv')
          from_email = os.environ.get('SENDGRID_FROM','loadtest-bot@example.com')

          body = (
              f"Repository: {repo}\n"
              f"Workflow: {workflow}\n"
              f"Run: #{run_number}\n"
              f"Run URL: {server}/{repo}/actions/runs/{run_id}\n\n"
              f"Status: {status}\n"
              f"Samples: {samples}\n"
              f"Errors: {errors}\n"
              f"Error %: {error_pct}\n"
              f"Format: {fmt}\n\n"
              "(Automated email)"
          )

          payload = {
              "personalizations": [{
                  "to": [{"email": r} for r in recips]
              }],
              "from": {"email": from_email},
              "subject": f"JMeter Load Test {status} | {repo} #{run_number}",
              "content": [{
                  "type": "text/plain",
                  "value": body
              }]
          }

          with open("payload.json", "w") as f:
              json.dump(payload, f)

          print("Payload size (bytes):", os.path.getsize("payload.json"))
          # No attachments included by design
          PY

          # 4. Send
          HTTP_CODE=$(curl -s -o resp.txt -w '%{http_code}' \
            -X POST https://api.sendgrid.com/v3/mail/send \
            -H "Authorization: Bearer ${SENDGRID_API_KEY}" \
            -H 'Content-Type: application/json' \
            --data-binary @payload.json) || { echo 'curl failed'; exit 1; }

          echo "HTTP code: $HTTP_CODE"
          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
            echo 'Email sent.'
          else
            echo 'Email failed (first 2KB of response):' >&2
            head -c 2048 resp.txt >&2 || true
            echo >&2
            exit 1
          fi

          rm -f resp.txt payload.json || true

      - name: Assemble artifacts bundle
        if: ${{ always() }}
        id: assemble
        run: |
          set -euo pipefail
          BUNDLE_BASE="artifacts_bundle"
          # Use RUN_ID if available to create unique folder (fallback to run number)
          RID="${RUN_ID:-${GITHUB_RUN_NUMBER}}"
            BUNDLE_DIR="${BUNDLE_BASE}/${RID}"
          mkdir -p "$BUNDLE_DIR"
          echo "Assembling artifacts into $BUNDLE_DIR"
          # Copy aggregated metrics files if present
          for f in aggregated_metrics.json agg_output.log; do
            [ -f "$f" ] && cp "$f" "$BUNDLE_DIR/" || true
          done
          # Copy Grafana snapshots directory
          if [ -d grafana_snapshots ]; then
            cp -R grafana_snapshots "$BUNDLE_DIR/" || true
          fi
          # Copy run directory content (flatten under run/ to avoid name collisions)
          RUN_DIR='${{ steps.locate.outputs.latest_dir }}'
          if [ -n "$RUN_DIR" ] && [ -d "$RUN_DIR" ]; then
            mkdir -p "$BUNDLE_DIR/run"
            cp -R "$RUN_DIR"/* "$BUNDLE_DIR/run/" 2>/dev/null || true
          else
            echo "Run directory missing ($RUN_DIR)"
          fi
          # Optional: include a manifest file summarizing contents
          {
            echo "run_id=${RID}"
            echo "generated_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            echo "github_run=${GITHUB_RUN_ID}"; echo "commit=${GITHUB_SHA}"; echo "branch=${GITHUB_REF_NAME}";
          } > "$BUNDLE_DIR/manifest.txt"
          echo "bundle_dir=$BUNDLE_DIR" >> $GITHUB_OUTPUT
          echo "Bundle contents:"; find "$BUNDLE_DIR" -maxdepth 2 -type f -printf '%P\n' || true

      - name: Upload artifacts bundle
        uses: actions/upload-artifact@v4
        with:
          name: loadtest-${{ steps.run_id.outputs.run_id || github.run_number }}
          path: ${{ steps.assemble.outputs.bundle_dir }}

      - name: Finish
        run: echo "Done."
